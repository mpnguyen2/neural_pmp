{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorised NoisyLinear layer with bias\n",
    "class NoisyLinear(nn.Module):\n",
    "  r\"\"\"\n",
    "  Used for better generalization and exploration from paper: \n",
    "  https://arxiv.org/abs/1706.10295?context=stat.ML\n",
    "\n",
    "  Args:\n",
    "    in_features (int): Input dimensions\n",
    "    out_features (int): Output dimensions\n",
    "    std_init (float): std for noise. More std means higher exploration\n",
    "  \"\"\"\n",
    "  def __init__(self, in_features, out_features, std_init=0.5):\n",
    "    super(NoisyLinear, self).__init__()\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features\n",
    "    self.std_init = std_init\n",
    "    self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "    self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "    self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n",
    "    self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "    self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "    self.register_buffer('bias_epsilon', torch.empty(out_features))\n",
    "    self.reset_parameters()\n",
    "    self.reset_noise()\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    mu_range = 1 / math.sqrt(self.in_features)\n",
    "    self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "    self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "    self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "    self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "\n",
    "  def _scale_noise(self, size):\n",
    "    x = torch.randn(size)\n",
    "    return x.sign().mul_(x.abs().sqrt_())\n",
    "\n",
    "  def reset_noise(self):\n",
    "    epsilon_in = self._scale_noise(self.in_features)\n",
    "    epsilon_out = self._scale_noise(self.out_features)\n",
    "    self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "    self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "  def forward(self, input):\n",
    "    if self.training:\n",
    "      return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon, self.bias_mu + self.bias_sigma * self.bias_epsilon)\n",
    "    else:\n",
    "      return F.linear(input, self.weight_mu, self.bias_mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple multi-layer perceptron net (densly connected net)\n",
    "    Args:\n",
    "        input_dim (int): Input dimension\n",
    "        output_dim (int): Output dimension\n",
    "        layer_dims (List[int]): Dimensions of hidden layers\n",
    "        activation (str): type of activations. Not applying to the last layer \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, layer_dims=[], activation='tanh'):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.layers = []\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        if len(layer_dims) != 0:\n",
    "            self.layers.append(NoisyLinear(input_dim, layer_dims[0]))\n",
    "            for i in range(len(layer_dims)-1):\n",
    "                if activation == 'tanh':\n",
    "                    self.layers.append(nn.Tanh())\n",
    "                elif activation == 'relu':\n",
    "                    self.layers.append(nn.ReLU())\n",
    "                self.layers.append(NoisyLinear(layer_dims[i], layer_dims[i+1]))\n",
    "            if activation == 'tanh':\n",
    "                self.layers.append(nn.Tanh())\n",
    "            elif activation == 'relu':\n",
    "                    self.layers.append(nn.ReLU())\n",
    "            self.layers.append(NoisyLinear(layer_dims[-1], output_dim))\n",
    "        else:\n",
    "            self.layers.append(NoisyLinear(input_dim, output_dim))\n",
    "        # Composing all layers\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    def reset_noise(self):\n",
    "        for name, module in self.named_children():\n",
    "            if 'fc' in name:\n",
    "                module.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "q_dim = 2; u_dim = 1\n",
    "\n",
    "adj_net = Mlp(input_dim=q_dim, output_dim=q_dim, layer_dims=[8, 16, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Testing\n",
    "print(adj_net)\n",
    "adj_net.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Displayer\n",
    "file = \"/home/jeffreymo572/Current/neural_pmp/output/optimal_traj_numpy/cartpole.npy\"\n",
    "img_array = np.load(file)\n",
    "\n",
    "print(img_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NeuralPMP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad171b39df7e4d1862965d71285e179cc9058524e25f5696c6911d6963862b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
